{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550cb994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 64) (18000, 64) (18000, 64) (18000,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "realCIR = np.load('real.npy')\n",
    "imagCIR = np.load('imag.npy')\n",
    "magCIR = np.load('dataset.npy')\n",
    "GT = np.load('label.npy')\n",
    "\n",
    "print(realCIR.shape, imagCIR.shape, magCIR.shape, GT.shape)\n",
    "\n",
    "tan = np.arctan2(imagCIR, realCIR)\n",
    "diff = tan[:,:63] - tan[:, 1:]\n",
    "diffrence = np.zeros((magCIR.shape[0],magCIR.shape[1]))\n",
    "diffrence[:,1:] = diff\n",
    "\n",
    "array = np.stack([magCIR,diffrence],axis=-1)\n",
    "# array = np.stack([magCIR,tan],axis=-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(array, GT, test_size=0.2, shuffle=True, stratify=GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10823e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convA1 = tf.keras.layers.Conv1D(64,7,2,activation=tf.nn.relu,input_shape=(64,2),padding='same')\n",
    "        self.normA1 = tf.keras.layers.BatchNormalization()\n",
    "        self.poolA1 = tf.keras.layers.MaxPooling1D(2,padding='same')\n",
    "        \n",
    "        self.convB1 = tf.keras.layers.Conv1D(64,3,1,padding='same')\n",
    "        self.convB2 = tf.keras.layers.Conv1D(64,3,1,padding='same')\n",
    "        self.convB3 = tf.keras.layers.Conv1D(64,3,1,padding='same')\n",
    "        self.convB4 = tf.keras.layers.Conv1D(64,3,1,padding='same')\n",
    "        self.normB1 = tf.keras.layers.BatchNormalization()\n",
    "        self.normB2 = tf.keras.layers.BatchNormalization()\n",
    "        self.poolB1 = tf.keras.layers.MaxPooling1D(2,padding='same')\n",
    "        \n",
    "        self.convC1 = tf.keras.layers.Conv1D(128,3,1,padding='same')\n",
    "        self.convC2 = tf.keras.layers.Conv1D(128,3,1,padding='same')\n",
    "        self.convC3 = tf.keras.layers.Conv1D(128,3,1,padding='same')\n",
    "        self.convC4 = tf.keras.layers.Conv1D(128,3,1,padding='same')\n",
    "        self.normC1 = tf.keras.layers.BatchNormalization()\n",
    "        self.normC2 = tf.keras.layers.BatchNormalization()\n",
    "        self.poolC1 = tf.keras.layers.MaxPooling1D(2,padding='same')\n",
    "        self.shortcutC = tf.keras.layers.Conv1D(128,1,1,padding='same')\n",
    "        \n",
    "        self.convD1 = tf.keras.layers.Conv1D(256,3,1,padding='same')\n",
    "        self.convD2 = tf.keras.layers.Conv1D(256,3,1,padding='same')\n",
    "        self.convD3 = tf.keras.layers.Conv1D(256,3,1,padding='same')\n",
    "        self.convD4 = tf.keras.layers.Conv1D(256,3,1,padding='same')\n",
    "        self.normD1 = tf.keras.layers.BatchNormalization()\n",
    "        self.normD2 = tf.keras.layers.BatchNormalization()\n",
    "        self.poolD1 = tf.keras.layers.MaxPooling1D(2,padding='same')\n",
    "        self.shortcutD = tf.keras.layers.Conv1D(256,1,1,padding='same')\n",
    "        \n",
    "        self.convE1 = tf.keras.layers.Conv1D(512,3,1,padding='same')\n",
    "        self.convE2 = tf.keras.layers.Conv1D(512,3,1,padding='same')\n",
    "        self.convE3 = tf.keras.layers.Conv1D(512,3,1,padding='same')\n",
    "        self.convE4 = tf.keras.layers.Conv1D(512,3,1,padding='same')\n",
    "        self.normE1 = tf.keras.layers.BatchNormalization()\n",
    "        self.normE2 = tf.keras.layers.BatchNormalization()\n",
    "        self.poolE1 = tf.keras.layers.MaxPooling1D(2,padding='same')\n",
    "        self.shortcutE = tf.keras.layers.Conv1D(512,1,1,padding='same')\n",
    "        \n",
    "        self.normF = tf.keras.layers.AveragePooling1D()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.denseF1 = tf.keras.layers.Dense(1000,activation=tf.nn.relu)\n",
    "        self.denseF2 = tf.keras.layers.Dense(6,activation=tf.nn.softmax)\n",
    "#     def conv_block(input_tensor):\n",
    "#         x = tf.keras.layers.Conv1D(64,3,1,padding='same')(input_tensor)\n",
    "#         x = tf.keras.layers.BatchNormalization()(x)\n",
    "#         x = tf.nn.relu(x)\n",
    "#         x = tf.keras.layers.Conv1D(64,3,1,padding='same')(x)\n",
    "#         x = tf.keras.layers.BatchNormalization()(x)\n",
    "#         sc = tf.keras.layers.Conv1D(64,3,1,padding='same')(input_tensor)\n",
    "#         sc = tf.keras.layers.BatchNormalization()(sc)\n",
    "#         return x+sc\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # blockA\n",
    "        x = self.convA1(inputs)\n",
    "        x = self.normA1(x)\n",
    "        x = self.poolA1(x)\n",
    "        # blockB\n",
    "        con_b = self.convB1(x)\n",
    "        con_b = self.normB1(con_b)\n",
    "        con_b = tf.keras.activations.relu(con_b)\n",
    "        con_b = self.convB2(con_b)\n",
    "        x += con_b\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        con_b = self.convB3(x)\n",
    "        con_b = self.normB2(con_b)\n",
    "        con_b = tf.keras.activations.relu(con_b)\n",
    "        con_b = self.convB4(con_b)\n",
    "        x += con_b\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.poolB1(x)\n",
    "        # blockC\n",
    "        con_c = self.convC1(x)\n",
    "        con_c = self.normC1(con_c)\n",
    "        con_c = tf.keras.activations.relu(con_c)\n",
    "        con_c = self.convC2(con_c)\n",
    "        x = self.shortcutC(x)\n",
    "        x += con_c\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        con_c = self.convC3(x)\n",
    "        con_c = self.normC2(con_c)\n",
    "        con_c = tf.keras.activations.relu(con_c)\n",
    "        con_c = self.convC4(con_c)\n",
    "        x += con_c\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.poolC1(x)\n",
    "        # blockD\n",
    "        con_d = self.convD1(x)\n",
    "        con_d = self.normD1(con_d)\n",
    "        con_d = tf.keras.activations.relu(con_d)\n",
    "        con_d = self.convD2(con_d)\n",
    "        x = self.shortcutD(x)\n",
    "        x += con_d\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        con_d = self.convD3(x)\n",
    "        con_d = self.normD2(con_d)\n",
    "        con_d = tf.keras.activations.relu(con_d)\n",
    "        con_d = self.convD4(con_d)\n",
    "        x += con_d\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.poolD1(x)\n",
    "        # blockE\n",
    "        con_e = self.convE1(x)\n",
    "        con_e = self.normE1(con_e)\n",
    "        con_e = tf.keras.activations.relu(con_e)\n",
    "        con_e = self.convE2(con_e)\n",
    "        x = self.shortcutE(x)\n",
    "        x += con_e\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        con_e = self.convE3(x)\n",
    "        con_e = self.normE1(con_e)\n",
    "        con_e = tf.keras.activations.relu(con_e)\n",
    "        con_e = self.convE4(con_e)\n",
    "        x += con_e\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        # blockF\n",
    "        x = self.normF(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.denseF1(x)\n",
    "        x = self.denseF2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a097141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 19:59:25.878980: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f9f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    start = 0.0001\n",
    "    drop = 0.1\n",
    "    epochs_drop = 5\n",
    "    lr = start * (drop ** np.floor((epoch)/epochs_drop))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a7ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(step_decay, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ca6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "189e1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape = (None,64,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e925f284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"res_net18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             multiple                  960       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  multiple                 256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           multiple                  12352     \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           multiple                  12352     \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           multiple                  12352     \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           multiple                  12352     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  multiple                 256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  multiple                 256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  multiple                 0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           multiple                  24704     \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           multiple                  49280     \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           multiple                  49280     \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           multiple                  49280     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  multiple                 512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  multiple                 512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  multiple                 0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           multiple                  8320      \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          multiple                  98560     \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          multiple                  196864    \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          multiple                  196864    \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          multiple                  196864    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  multiple                 1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  multiple                 1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  multiple                 0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          multiple                  33024     \n",
      "                                                                 \n",
      " conv1d_15 (Conv1D)          multiple                  393728    \n",
      "                                                                 \n",
      " conv1d_16 (Conv1D)          multiple                  786944    \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          multiple                  786944    \n",
      "                                                                 \n",
      " conv1d_18 (Conv1D)          multiple                  786944    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  multiple                 2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  multiple                 0 (unused)\n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  multiple                 0 (unused)\n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          multiple                  131584    \n",
      "                                                                 \n",
      " average_pooling1d (AverageP  multiple                 0         \n",
      " ooling1D)                                                       \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  513000    \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  6006      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,364,446\n",
      "Trainable params: 4,361,502\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28761531",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/10\n",
      "450/450 [==============================] - 25s 54ms/step - loss: 0.9824 - accuracy: 0.6008 - val_loss: 0.8659 - val_accuracy: 0.6600 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 2/10\n",
      "450/450 [==============================] - 24s 53ms/step - loss: 0.6282 - accuracy: 0.7547 - val_loss: 0.7451 - val_accuracy: 0.7094 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 3/10\n",
      "450/450 [==============================] - 25s 55ms/step - loss: 0.4837 - accuracy: 0.8099 - val_loss: 0.6658 - val_accuracy: 0.7511 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 4/10\n",
      "450/450 [==============================] - 24s 52ms/step - loss: 0.3890 - accuracy: 0.8536 - val_loss: 0.7353 - val_accuracy: 0.7408 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 5/10\n",
      "450/450 [==============================] - 24s 52ms/step - loss: 0.3123 - accuracy: 0.8820 - val_loss: 0.7472 - val_accuracy: 0.7406 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 6/10\n",
      "450/450 [==============================] - 24s 54ms/step - loss: 0.1605 - accuracy: 0.9451 - val_loss: 0.6049 - val_accuracy: 0.7933 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 7/10\n",
      "450/450 [==============================] - 25s 57ms/step - loss: 0.1141 - accuracy: 0.9606 - val_loss: 0.6347 - val_accuracy: 0.7958 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 8/10\n",
      "450/450 [==============================] - 26s 57ms/step - loss: 0.0906 - accuracy: 0.9700 - val_loss: 0.6654 - val_accuracy: 0.7942 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 9/10\n",
      "450/450 [==============================] - 24s 53ms/step - loss: 0.0705 - accuracy: 0.9764 - val_loss: 0.7142 - val_accuracy: 0.7897 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 10/10\n",
      "450/450 [==============================] - 23s 52ms/step - loss: 0.0582 - accuracy: 0.9813 - val_loss: 0.7599 - val_accuracy: 0.7886 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9ab10f400>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10,\n",
    "            validation_data=(X_test, y_test),\n",
    "             callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0068226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 1s 9ms/step - loss: 0.7599 - accuracy: 0.7886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7598958611488342, 0.788611114025116]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ba280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor,filters,kernel_size,strides):\n",
    "    x = tf.keras.layers.Conv1D(filters,kernel_size,strides,padding='same')(input_tensor)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.keras.layers.Conv1D(filters,kernel_size,strides,padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    return x + input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def conv_block(input_tensor):\n",
    "        x = tf.keras.layers.Conv1D(64,3,1,padding='same')(input_tensor)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.keras.layers.Conv1D(64,3,1,padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        sc = tf.keras.layers.Conv1D(64,3,1,padding='same')(input_tensor)\n",
    "        sc = tf.keras.layers.BatchNormalization()(sc)\n",
    "        return x+sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78405901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor,filters,kernel_size,strides):\n",
    "    x = tf.keras.layers.Conv1D(filters,kernel_size,strides,padding='same')(input_tensor)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.keras.layers.Conv1D(filters,kernel_size,strides,padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    sc = tf.keras.layers.Conv1D(filters,kernel_size,strides,padding='same')(input_tensor)\n",
    "    sc = tf.keras.layers.BatchNormalization()(sc)\n",
    "    return x+sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54549486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ResNet50 model for Keras.\n",
    "\n",
    "# Reference:\n",
    "\n",
    "- [Deep Residual Learning for Image Recognition](\n",
    "    https://arxiv.org/abs/1512.03385) (CVPR 2016 Best Paper Award)\n",
    "\n",
    "Adapted from code contributed by BigMoyan.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from . import get_submodules_from_kwargs\n",
    "from . import imagenet_utils\n",
    "from .imagenet_utils import decode_predictions\n",
    "from .imagenet_utils import _obtain_input_shape\n",
    "\n",
    "preprocess_input = imagenet_utils.preprocess_input\n",
    "\n",
    "WEIGHTS_PATH = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                'releases/download/v0.2/'\n",
    "                'resnet50_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "WEIGHTS_PATH_NO_TOP = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                       'releases/download/v0.2/'\n",
    "                       'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "backend = None\n",
    "layers = None\n",
    "models = None\n",
    "keras_utils = None\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size,\n",
    "                      padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters3, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2c')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        strides: Strides for the first conv layer in the block.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "\n",
    "    Note that from stage 3,\n",
    "    the first conv layer at main path is with strides=(2, 2)\n",
    "    And the shortcut should have strides=(2, 2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1), strides=strides,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size, padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters3, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2c')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = layers.Conv2D(filters3, (1, 1), strides=strides,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = layers.BatchNormalization(\n",
    "        axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def ResNet50(include_top=True,\n",
    "             weights='imagenet',\n",
    "             input_tensor=None,\n",
    "             input_shape=None,\n",
    "             pooling=None,\n",
    "             classes=1000,\n",
    "             **kwargs):\n",
    "    \"\"\"Instantiates the ResNet50 architecture.\n",
    "\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "\n",
    "    # Arguments\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 32.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional block.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional block, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    global backend, layers, models, keras_utils\n",
    "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=32,\n",
    "                                      data_format=backend.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n",
    "    x = layers.Conv2D(64, (7, 7),\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name='conv1')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='fc1000')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "        else:\n",
    "            warnings.warn('The output shape of `ResNet50(include_top=False)` '\n",
    "                          'has been changed since Keras 2.2.0.')\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='resnet50')\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'resnet50_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                WEIGHTS_PATH,\n",
    "                cache_subdir='models',\n",
    "                md5_hash='a7b3fe01876f51b976af0dea6bc144eb')\n",
    "        else:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                md5_hash='a268eb855778b3df3c7506639542a6af')\n",
    "        model.load_weights(weights_path)\n",
    "        if backend.backend() == 'theano':\n",
    "            keras_utils.convert_all_kernels_in_model(model)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
